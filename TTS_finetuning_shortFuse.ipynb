{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50T0jskpkv2h"
      },
      "source": [
        "## Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TxbE17sEpBLL",
        "outputId": "fda215bf-b569-492a-ed31-c62564040b11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hydra-core==1.1 in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core==1.1) (5.8.0)\n",
            "Requirement already satisfied: omegaconf==2.1.* in /usr/local/lib/python3.7/dist-packages (from hydra-core==1.1) (2.1.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.7/dist-packages (from hydra-core==1.1) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from omegaconf==2.1.*->hydra-core==1.1) (6.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core==1.1) (3.8.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy==1.7.3 in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy==1.7.3) (1.21.6)\n",
            "E: dpkg was interrupted, you must manually run 'dpkg --configure -a' to correct the problem. \n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 15.5 MB/s \n",
            "\u001b[?25hCollecting pynini==2.1.4\n",
            "  Downloading pynini-2.1.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (165.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 165.5 MB 48 kB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from pynini==2.1.4) (0.29.30)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=8691691e7bc7395641121b7e69e505ea6e66db8008f64a9223eae40ee0cc4fa7\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, unidecode, pynini\n",
            "Successfully installed pynini-2.1.4 unidecode-1.3.4 wget-3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nemo_toolkit[all]\n",
            "  Cloning https://github.com/NVIDIA/NeMo.git (to revision main) to /tmp/pip-install-0i8c585h/nemo-toolkit_42ee5123aadc42009cab84547feaa728\n",
            "  Running command git clone -q https://github.com/NVIDIA/NeMo.git /tmp/pip-install-0i8c585h/nemo-toolkit_42ee5123aadc42009cab84547feaa728\n",
            "Collecting setuptools==59.5.0\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[K     |████████████████████████████████| 952 kB 15.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (1.21.6)\n",
            "Collecting onnx>=1.7.0\n",
            "  Downloading onnx-1.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 62.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (2.8.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (1.12.0+cu113)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (1.14.1)\n",
            "Collecting ruamel.yaml\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 85.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (1.0.2)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (4.64.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (0.51.2)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (3.2)\n",
            "Collecting frozendict\n",
            "  Downloading frozendict-2.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 10.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (1.3.4)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 13.3 MB/s \n",
            "\u001b[?25hCollecting black==19.10b0\n",
            "  Downloading black-19.10b0-py36-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting isort[requirements]<5\n",
            "  Downloading isort-4.3.21-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting parameterized\n",
            "  Downloading parameterized-0.8.1-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (3.6.4)\n",
            "Collecting pytest-runner\n",
            "  Using cached pytest_runner-6.0.0-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (1.8.6)\n",
            "Collecting sphinxcontrib-bibtex\n",
            "  Downloading sphinxcontrib_bibtex-2.4.2-py3-none-any.whl (39 kB)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 68.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (2.1.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (2022.6.2)\n",
            "Requirement already satisfied: pynini==2.1.4 in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (2.1.4)\n",
            "Collecting transformers>=4.0.1\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 77.0 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning>=1.6.1\n",
            "  Downloading pytorch_lightning-1.6.5-py3-none-any.whl (585 kB)\n",
            "\u001b[K     |████████████████████████████████| 585 kB 82.7 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.4.1rc0\n",
            "  Downloading torchmetrics-0.9.2-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 89.8 MB/s \n",
            "\u001b[?25hCollecting webdataset<=0.1.62,>=0.1.48\n",
            "  Downloading webdataset-0.1.62-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: omegaconf<2.2,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (2.1.2)\n",
            "Requirement already satisfied: hydra-core<1.2,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (1.1.0)\n",
            "Collecting pyyaml<6\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 88.8 MB/s \n",
            "\u001b[?25hCollecting sentencepiece<1.0.0\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 63.5 MB/s \n",
            "\u001b[?25hCollecting youtokentome>=1.0.5\n",
            "  Downloading youtokentome-1.0.6-cp37-cp37m-manylinux2010_x86_64.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 76.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (1.3.5)\n",
            "Collecting sacremoses>=0.0.43\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 89.2 MB/s \n",
            "\u001b[?25hCollecting braceexpand\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (0.5.3)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (0.8.1)\n",
            "Collecting marshmallow\n",
            "  Downloading marshmallow-3.17.0-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (21.3)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (0.10.3.post1)\n",
            "Collecting sox\n",
            "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
            "Collecting kaldi-python-io\n",
            "  Downloading kaldi-python-io-1.2.2.tar.gz (8.8 kB)\n",
            "Collecting kaldiio\n",
            "  Downloading kaldiio-2.17.2.tar.gz (24 kB)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (1.7.3)\n",
            "Collecting g2p_en\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 73.7 MB/s \n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting pyannote.core\n",
            "  Downloading pyannote.core-4.4-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting pyannote.metrics\n",
            "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 193 kB/s \n",
            "\u001b[?25hRequirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (7.7.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (3.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (7.1.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (0.13.0+cu113)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.24.34-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 65.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (3.1.0)\n",
            "Collecting matplotlib>=3.3.2\n",
            "  Downloading matplotlib-3.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 73.1 MB/s \n",
            "\u001b[?25hCollecting rapidfuzz\n",
            "  Downloading rapidfuzz-2.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 51.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (4.4.0)\n",
            "Collecting sacrebleu[ja]\n",
            "  Downloading sacrebleu-2.1.0-py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 12.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (3.7)\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting opencc\n",
            "  Downloading OpenCC-1.1.4-cp37-cp37m-manylinux1_x86_64.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 78.1 MB/s \n",
            "\u001b[?25hCollecting pangu\n",
            "  Downloading pangu-4.0.6.1-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from nemo_toolkit[all]) (0.42.1)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting flask_restful\n",
            "  Downloading Flask_RESTful-0.3.9-py2.py3-none-any.whl (25 kB)\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Collecting ijson\n",
            "  Downloading ijson-3.1.4-cp37-cp37m-manylinux2010_x86_64.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 82.1 MB/s \n",
            "\u001b[?25hCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 43.4 MB/s \n",
            "\u001b[?25hCollecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting pypinyin\n",
            "  Downloading pypinyin-0.46.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 55.7 MB/s \n",
            "\u001b[?25hCollecting attrdict\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Collecting pystoi\n",
            "  Downloading pystoi-0.3.3.tar.gz (7.0 kB)\n",
            "Collecting pesq\n",
            "  Downloading pesq-0.0.4.tar.gz (38 kB)\n",
            "Collecting toml>=0.9.4\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click>=6.5 in /usr/local/lib/python3.7/dist-packages (from black==19.10b0->nemo_toolkit[all]) (7.1.2)\n",
            "Collecting pathspec<1,>=0.6\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from black==19.10b0->nemo_toolkit[all]) (21.4.0)\n",
            "Collecting typed-ast>=1.4.0\n",
            "  Downloading typed_ast-1.5.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
            "\u001b[K     |████████████████████████████████| 843 kB 73.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==19.10b0->nemo_toolkit[all]) (1.4.4)\n",
            "Requirement already satisfied: Cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from pynini==2.1.4->nemo_toolkit[all]) (0.29.30)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.2,>=1.1.0->nemo_toolkit[all]) (5.8.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.2,>=1.1.0->nemo_toolkit[all]) (4.8)\n",
            "Collecting pipreqs\n",
            "  Downloading pipreqs-0.4.11-py2.py3-none-any.whl (32 kB)\n",
            "Collecting pip-api\n",
            "  Downloading pip_api-0.0.30-py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 76.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.7.0->nemo_toolkit[all]) (4.1.1)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.7.0->nemo_toolkit[all]) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.12.2->onnx>=1.7.0->nemo_toolkit[all]) (1.15.0)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 88.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.6.1->nemo_toolkit[all]) (2.8.0)\n",
            "Collecting pyDeprecate>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 74.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->nemo_toolkit[all]) (3.0.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses>=0.0.43->nemo_toolkit[all]) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (1.1.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (1.35.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (1.47.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.1->nemo_toolkit[all]) (3.7.1)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 76.8 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (2.1.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 80.7 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 94.2 MB/s \n",
            "\u001b[?25hCollecting botocore<1.28.0,>=1.27.34\n",
            "  Downloading botocore-1.27.34-py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 79.2 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.7 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 99.4 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.0-py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask_restful->nemo_toolkit[all]) (1.1.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from flask_restful->nemo_toolkit[all]) (2022.1)\n",
            "Collecting aniso8601>=0.82\n",
            "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_restful->nemo_toolkit[all]) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_restful->nemo_toolkit[all]) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask_restful->nemo_toolkit[all]) (2.0.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->nemo_toolkit[all]) (0.2.5)\n",
            "Collecting distance>=0.1.3\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 86.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown->nemo_toolkit[all]) (4.6.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->nemo_toolkit[all]) (1.5.2)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->nemo_toolkit[all]) (1.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->nemo_toolkit[all]) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->nemo_toolkit[all]) (5.1.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->nemo_toolkit[all]) (4.10.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->nemo_toolkit[all]) (3.6.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->nemo_toolkit[all]) (0.2.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit[all]) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit[all]) (5.3.5)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (1.0.18)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (5.3.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (5.4.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (1.8.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (5.6.1)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (4.11.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.13.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->nemo_toolkit[all]) (23.2.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.7.0)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa->nemo_toolkit[all]) (1.6.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa->nemo_toolkit[all]) (0.3.1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->nemo_toolkit[all]) (2.1.9)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->nemo_toolkit[all]) (0.34.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->nemo_toolkit[all]) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile->nemo_toolkit[all]) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile->nemo_toolkit[all]) (2.21)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->nemo_toolkit[all]) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->nemo_toolkit[all]) (1.4.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (5.0.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (2.15.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.18.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.5.1)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from pip-api->isort[requirements]<5->nemo_toolkit[all]) (21.1.3)\n",
            "Collecting yarg\n",
            "  Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from pipreqs->isort[requirements]<5->nemo_toolkit[all]) (0.6.2)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from pyannote.core->nemo_toolkit[all]) (2.4.0)\n",
            "Collecting simplejson>=3.8.1\n",
            "  Downloading simplejson-3.17.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 87.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from pyannote.metrics->nemo_toolkit[all]) (0.8.10)\n",
            "Requirement already satisfied: sympy>=1.1 in /usr/local/lib/python3.7/dist-packages (from pyannote.metrics->nemo_toolkit[all]) (1.7.1)\n",
            "Collecting pyannote.database>=4.0.1\n",
            "  Downloading pyannote.database-4.1.3-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 514 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typer[all]>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[all]) (0.4.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy>=1.1->pyannote.metrics->nemo_toolkit[all]) (1.2.1)\n",
            "Collecting shellingham<2.0.0,>=1.3.0\n",
            "  Downloading shellingham-1.4.0-py2.py3-none-any.whl (9.4 kB)\n",
            "Collecting colorama<0.5.0,>=0.4.3\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->nemo_toolkit[all]) (8.13.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->nemo_toolkit[all]) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->nemo_toolkit[all]) (1.11.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->nemo_toolkit[all]) (1.4.1)\n",
            "Collecting jarowinkler<2.0.0,>=1.2.0\n",
            "  Downloading jarowinkler-1.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 91.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.6.1->nemo_toolkit[all]) (1.7.1)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 84.3 MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting mecab-python3==1.0.5\n",
            "  Downloading mecab_python3-1.0.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (574 kB)\n",
            "\u001b[K     |████████████████████████████████| 574 kB 84.9 MB/s \n",
            "\u001b[?25hCollecting ipadic<2.0,>=1.0\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.4 MB 78.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx->nemo_toolkit[all]) (1.4.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx->nemo_toolkit[all]) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx->nemo_toolkit[all]) (1.2.4)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx->nemo_toolkit[all]) (2.10.3)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx->nemo_toolkit[all]) (2.2.0)\n",
            "Requirement already satisfied: docutils<0.18,>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx->nemo_toolkit[all]) (0.17.1)\n",
            "Collecting pybtex-docutils>=1.0.0\n",
            "  Downloading pybtex_docutils-1.0.2-py3-none-any.whl (6.3 kB)\n",
            "Collecting pybtex>=0.24\n",
            "  Downloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "\u001b[K     |████████████████████████████████| 561 kB 69.7 MB/s \n",
            "\u001b[?25hCollecting sphinx\n",
            "  Downloading Sphinx-5.0.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 72.3 MB/s \n",
            "\u001b[?25hCollecting latexcodec>=1.0.4\n",
            "  Downloading latexcodec-2.0.1-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx->nemo_toolkit[all]) (1.1.5)\n",
            "Collecting sphinxcontrib-qthelp\n",
            "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 12.5 MB/s \n",
            "\u001b[?25hCollecting sphinxcontrib-jsmath\n",
            "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
            "Collecting sphinxcontrib-htmlhelp>=2.0.0\n",
            "  Downloading sphinxcontrib_htmlhelp-2.0.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 12.0 MB/s \n",
            "\u001b[?25hCollecting sphinxcontrib-devhelp\n",
            "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting sphinxcontrib-applehelp\n",
            "  Downloading sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 90.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->nemo_toolkit[all]) (5.4.8)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->nemo_toolkit[all]) (2.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.8.0-py2.py3-none-any.whl (153 kB)\n",
            "\u001b[K     |████████████████████████████████| 153 kB 78.5 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 94.1 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: nemo-toolkit, sacremoses, fasttext, distance, kaldi-python-io, kaldiio, pesq, pystoi, ipadic, sentence-transformers, pathtools\n",
            "  Building wheel for nemo-toolkit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nemo-toolkit: filename=nemo_toolkit-1.11.0rc0-py3-none-any.whl size=3653096 sha256=932d51ee4fce1d65fef4622d9a4527de95a69ab6f645a5399750124b076b09db\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-m32fssqn/wheels/82/66/ef/7248c1c27216d06425cab14b3391059534c889aaf2db89b855\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=9f40704efbbb5382b7935507d86ae2d641497f133ea1ee0e200e4fc279b58407\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3166270 sha256=f6f667248093fab404826aa52ae9603a1b78ef1413873ccd2513244ee5cbe901\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16276 sha256=f6e735acede32e9c483700d49d972b02cc3ddc995ddd8d9eeebcdb7fb9bd083c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/10/1b/96fca621a1be378e2fe104cfb0d160bb6cdf3d04a3d35266cc\n",
            "  Building wheel for kaldi-python-io (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaldi-python-io: filename=kaldi_python_io-1.2.2-py3-none-any.whl size=8969 sha256=403cdeb7afda01aa2b8b4c411e7bd26d72b9a03eac84c248a6a5ac8afdff9b27\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/26/38/7678d1ff6cd1bbcbfc0d80b0a29d94d917dfa9ad790b4a85a9\n",
            "  Building wheel for kaldiio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaldiio: filename=kaldiio-2.17.2-py3-none-any.whl size=24471 sha256=f4a2956cb6b28590ef015c045c94c8eb3ecc6faea9fdbcac91df1b8ff284cf23\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/07/e8/45641287c59bf6ce41e22259f8680b521c31e6306cb88392ac\n",
            "  Building wheel for pesq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pesq: filename=pesq-0.0.4-cp37-cp37m-linux_x86_64.whl size=214575 sha256=68b67260b48bead7230699a26a802be4a0e6910c670983f244d3d1489de809c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/3d/9c/542731f8357f7c82eb6ac2047cc5375f92c9a05b09a715aff6\n",
            "  Building wheel for pystoi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pystoi: filename=pystoi-0.3.3-py2.py3-none-any.whl size=7793 sha256=532c2584dd48c811be4ccd82dfe27abae9a9e8e1a5cab12f33f0c42e7756ca1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/4a/ad/3ab460193ed0535430b4b1575f255aa6bae69df17453628e86\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556723 sha256=4acc73f14b8b34b07f676199984489353269c6bc9865d2446794f798999a746b\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/8b/99/cf0d27191876637cd3639a560f93aa982d7855ce826c94348b\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=ddbaec80588e17b4cf6a6fb5a5995087d1c82b0a72b504e6449d2e3167ffad97\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=fe0683912ce87950d53dab461514751fb8e8cb0f2c3cb0870254e27f10ec61b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built nemo-toolkit sacremoses fasttext distance kaldi-python-io kaldiio pesq pystoi ipadic sentence-transformers pathtools\n",
            "Installing collected packages: urllib3, setuptools, multidict, frozenlist, yarl, smmap, simplejson, shellingham, pyyaml, latexcodec, jmespath, colorama, asynctest, async-timeout, aiosignal, yarg, tokenizers, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, ruamel.yaml.clib, pybtex, pyannote.core, portalocker, huggingface-hub, gitdb, fsspec, botocore, aiohttp, typed-ast, transformers, torchmetrics, toml, sphinx, shortuuid, setproctitle, sentry-sdk, sentencepiece, sacrebleu, s3transfer, ruamel.yaml, pyDeprecate, pybtex-docutils, pybind11, pyannote.database, pipreqs, pip-api, pathtools, pathspec, onnx, mecab-python3, jarowinkler, isort, ipadic, GitPython, frozendict, docker-pycreds, distance, braceexpand, aniso8601, youtokentome, webdataset, wandb, sphinxcontrib-bibtex, sox, sentence-transformers, sacremoses, rapidfuzz, pytorch-lightning, pytest-runner, pystoi, pypinyin, pydub, pyannote.metrics, pesq, parameterized, pangu, opencc, nemo-toolkit, marshmallow, kaldiio, kaldi-python-io, ijson, g2p-en, ftfy, flask-restful, fasttext, faiss-cpu, einops, boto3, black, attrdict\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 1.8.6\n",
            "    Uninstalling Sphinx-1.8.6:\n",
            "      Successfully uninstalled Sphinx-1.8.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.27 aiohttp-3.8.1 aiosignal-1.2.0 aniso8601-9.0.1 async-timeout-4.0.2 asynctest-0.13.0 attrdict-2.0.1 black-19.10b0 boto3-1.24.34 botocore-1.27.34 braceexpand-0.1.7 colorama-0.4.5 distance-0.1.3 docker-pycreds-0.4.0 einops-0.4.1 faiss-cpu-1.7.2 fasttext-0.9.2 flask-restful-0.3.9 frozendict-2.3.2 frozenlist-1.3.0 fsspec-2022.5.0 ftfy-6.1.1 g2p-en-2.1.0 gitdb-4.0.9 huggingface-hub-0.8.1 ijson-3.1.4 ipadic-1.0.0 isort-4.3.21 jarowinkler-1.2.0 jmespath-1.0.1 kaldi-python-io-1.2.2 kaldiio-2.17.2 latexcodec-2.0.1 marshmallow-3.17.0 mecab-python3-1.0.5 multidict-6.0.2 nemo-toolkit-1.11.0rc0 onnx-1.12.0 opencc-1.1.4 pangu-4.0.6.1 parameterized-0.8.1 pathspec-0.9.0 pathtools-0.1.2 pesq-0.0.4 pip-api-0.0.30 pipreqs-0.4.11 portalocker-2.5.1 pyDeprecate-0.3.2 pyannote.core-4.4 pyannote.database-4.1.3 pyannote.metrics-3.2.1 pybind11-2.10.0 pybtex-0.24.0 pybtex-docutils-1.0.2 pydub-0.25.1 pypinyin-0.46.0 pystoi-0.3.3 pytest-runner-6.0.0 pytorch-lightning-1.6.5 pyyaml-5.4.1 rapidfuzz-2.2.0 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 s3transfer-0.6.0 sacrebleu-2.1.0 sacremoses-0.0.53 sentence-transformers-2.2.2 sentencepiece-0.1.96 sentry-sdk-1.8.0 setproctitle-1.2.3 setuptools-59.5.0 shellingham-1.4.0 shortuuid-1.0.9 simplejson-3.17.6 smmap-5.0.0 sox-1.4.1 sphinx-5.0.2 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-bibtex-2.4.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 tokenizers-0.12.1 toml-0.10.2 torchmetrics-0.9.2 transformers-4.20.1 typed-ast-1.5.4 urllib3-1.25.11 wandb-0.12.21 webdataset-0.1.62 yarg-0.1.9 yarl-1.7.2 youtokentome-1.0.6\n",
            "Setting up libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "sox is already the newest version (14.4.2-3ubuntu0.18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Cloning into '/content/synthbot'...\n",
            "remote: Enumerating objects: 680, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 680 (delta 1), reused 1 (delta 1), pack-reused 674\u001b[K\n",
            "Receiving objects: 100% (680/680), 5.35 MiB | 18.20 MiB/s, done.\n",
            "Resolving deltas: 100% (308/308), done.\n",
            "Branch 'experimental' set up to track remote branch 'experimental' from 'origin'.\n",
            "Switched to a new branch 'experimental'\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pysoundfile\n",
            "  Downloading PySoundFile-0.9.0.post1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.7/dist-packages (from pysoundfile) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=0.6->pysoundfile) (2.21)\n",
            "Installing collected packages: pysoundfile\n",
            "Successfully installed pysoundfile-0.9.0.post1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.3.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.7.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (59.5.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (3.0.9)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n"
          ]
        }
      ],
      "source": [
        "!pip install hydra-core==1.1\n",
        "!pip install scipy==1.7.3\n",
        "\n",
        "BRANCH = 'main'\n",
        "# If you're using Google Colab and not running locally, uncomment and run this cell.\n",
        "!apt-get install sox libsndfile1 ffmpeg\n",
        "!pip install wget unidecode pynini==2.1.4\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
        "\n",
        "#=== load the repo and data (Thanks Synthbot) ===\n",
        "!sudo dpkg --configure -a\n",
        "!sudo apt -qq install -y sox\n",
        "!git clone \"https://github.com/synthbot-anon/synthbot.git\" /content/synthbot\n",
        "!(cd /content/synthbot; git checkout experimental)\n",
        "!python3 -m pip install pysoundfile \n",
        "!python3 -m pip install librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQfBIsMrHeGC",
        "outputId": "94cd49d2-0e69-4ab0-80c8-fb5f3385ceb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jul 21 14:47:31 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS0PkhnjpTjr",
        "outputId": "cc0a942d-d1bd-4b8b-f89b-76c6b7c17be2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n"
          ]
        }
      ],
      "source": [
        "home_path = !(echo $HOME)\n",
        "home_path = home_path[0]\n",
        "print(home_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap_5CDnqk7Pf"
      },
      "source": [
        "## Preparing the Dataset for Fastpitch finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsAc_3nkwpqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "866cd1f9-d70c-424c-8b2b-29bd0b8fb198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-21 14:47:56--  https://docs.google.com/uc?export=download&confirm=t&id=12ClseQAzkqsqCNMClmptvbSjfs2Ulcei\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.18.139, 142.251.18.101, 142.251.18.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.18.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0c-84-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/aeru0snkvqeuogeidoe3fl72ddd2bb8i/1658414850000/05434657448262868821/*/12ClseQAzkqsqCNMClmptvbSjfs2Ulcei?e=download&uuid=68fd87e4-4790-4b0a-8825-dc762198af00 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-07-21 14:47:57--  https://doc-0c-84-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/aeru0snkvqeuogeidoe3fl72ddd2bb8i/1658414850000/05434657448262868821/*/12ClseQAzkqsqCNMClmptvbSjfs2Ulcei?e=download&uuid=68fd87e4-4790-4b0a-8825-dc762198af00\n",
            "Resolving doc-0c-84-docs.googleusercontent.com (doc-0c-84-docs.googleusercontent.com)... 108.177.126.132, 2a00:1450:4013:c01::84\n",
            "Connecting to doc-0c-84-docs.googleusercontent.com (doc-0c-84-docs.googleusercontent.com)|108.177.126.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1355008000 (1.3G) [application/x-tar]\n",
            "Saving to: ‘applejack.tar’\n",
            "\n",
            "applejack.tar       100%[===================>]   1.26G  79.5MB/s    in 15s     \n",
            "\n",
            "2022-07-21 14:48:12 (87.2 MB/s) - ‘applejack.tar’ saved [1355008000/1355008000]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/resampy/interpn.py:114: NumbaWarning: \u001b[1m\u001b[1mThe TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\u001b[0m\u001b[0m\n",
            "  _resample_loop_p(x, t_out, interp_win, interp_delta, num_table, scale, y)\n"
          ]
        }
      ],
      "source": [
        "#downloading the dataset\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=15fFK30RicOIxoJwX1HXz7_7yavr6uBMq' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=15fFK30RicOIxoJwX1HXz7_7yavr6uBMq\" -O shortFuse.tar && rm -rf /tmp/cookies.txt\n",
        "\n",
        "skip_noisy = True # Disable to train with Noisy Data included\n",
        "allowed_emotions = \"\"\"\n",
        "Anxious\n",
        "Angry\n",
        "Annoyed\n",
        "Amused\n",
        "Confused\n",
        "Crazy\n",
        "Disgust\n",
        "Exhausted\n",
        "Fear\n",
        "Happy\n",
        "Neutral\n",
        "Sad\n",
        "Serious\n",
        "Singing\n",
        "Shouting\n",
        "Surprised\n",
        "Smug\n",
        "Love\n",
        "Sarcastic\n",
        "Tired\n",
        "Whispering\n",
        "Whining\n",
        "\"\"\".split(\"\\n\")[1:-1]\n",
        "\n",
        "percentage_training_data = 0.95 # 90% of Data will be used for training, 5% for Validation\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/synthbot/src')\n",
        "from ponysynth.corpus import ClipperArchive, phoneme_transcription\n",
        "import librosa\n",
        "import subprocess\n",
        "\n",
        "archive_fn = \"/content/shortFuse.tar\"\n",
        "archive = ClipperArchive(archive_fn)\n",
        "\n",
        "data_path = 'shortFuse'\n",
        "allowed_emotions = [x.lower() for x in allowed_emotions]\n",
        "!mkdir {data_path}\n",
        "!mkdir {data_path+\"/out\"}\n",
        "all_clips = []; all_clips_arpa = []; skipped_count=0; too_short_count=0; emotion_skip=0\n",
        "for key in archive.keys(): # write the audio files for processing in bash terminal\n",
        "  audio = archive.read_audio(key)\n",
        "  audio_fn = '{}/{}.wav'.format(data_path, key)\n",
        "  audio_fn_ = '{}/{}.wav'.format(data_path+\"/out\", key)\n",
        "  with open(audio_fn, 'wb') as audio_out:\n",
        "    audio_out.write(audio.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7sQiiTXJOZV"
      },
      "outputs": [],
      "source": [
        "%%script bash\n",
        "# trim all 48Khz files\n",
        "cd shortFuse\n",
        "for input in *.wav; do\n",
        "  output=\"out/$input\"\n",
        "  sox \"$input\" \"$output\" silence 1 0.05 0.1% reverse silence 1 0.05 0.1% reverse;\n",
        "done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OEmcvYyJOVI",
        "outputId": "9aecf5de-cd17-4147-aa15-cdfd222ee2f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2768 Files are too Noisy.\n",
            "0 Files contain an emotion not in permitted emotions\n",
            "203 Files are too short\n",
            "2486 Files kept in dataset.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "should_continue = 0 # should run \"continue\" command inside outer loop.\n",
        "for key in archive.keys():\n",
        "  label = archive.read_label(key)\n",
        "  audio_fn = '{}/{}.wav'.format(data_path, key)\n",
        "  audio_fn_ = '{}/{}.wav'.format(data_path+\"/out\", key)\n",
        "  if (label['noise'] in ['Very Noisy','Noisy']) and skip_noisy: os.remove(audio_fn_); skipped_count+=1; continue\n",
        "\n",
        "  for tag in label['tags']:\n",
        "     if tag.lower() not in allowed_emotions:\n",
        "       try: os.remove(audio_fn_)\n",
        "       except: pass\n",
        "       print(tag+\" emotion not in list\"); emotion_skip+=1; should_continue = 1; break # this is supposed to break the outer loop\n",
        "  if should_continue: should_continue = 0; continue\n",
        "\n",
        "  audio = archive.read_audio(key)\n",
        "  transcript = label['utterance']['content']\n",
        "  \n",
        "  if os.stat(audio_fn_).st_size < 71602:\n",
        "    #print(\"Skipping Audio, Duration: \"+str(len(librosa.core.load(audio_fn_, sr=48000)[0])/48000))\n",
        "    try: os.remove(audio_fn_)\n",
        "    except: pass\n",
        "    too_short_count+=1; continue # Skips files based on size.\n",
        "\n",
        "  filelist_line = \"{}|{}\".format(audio_fn_, transcript)\n",
        "  all_clips.append(filelist_line)\n",
        "  filelist_line = \"{}|{}\".format(audio_fn_, phoneme_transcription(label))\n",
        "  all_clips_arpa.append(filelist_line)\n",
        "    \n",
        "print(str(skipped_count)+\" Files are too Noisy.\")\n",
        "print(str(emotion_skip)+\" Files contain an emotion not in permitted emotions\")\n",
        "print(str(too_short_count)+\" Files are too short\")\n",
        "print(str(len(list(archive.keys()))-(skipped_count+too_short_count+emotion_skip))+\" Files kept in dataset.\")\n",
        "\n",
        "\n",
        "####################################################################################################################\n",
        "# shuffle the training data\n",
        "import random\n",
        "random.seed(0)\n",
        "random.shuffle(all_clips)\n",
        "\n",
        "# get train, test, validation splits\n",
        "num_clips = len(all_clips)\n",
        "train_end = int(num_clips * percentage_training_data)\n",
        "\n",
        "train = all_clips[:train_end]\n",
        "validation = all_clips[train_end:]\n",
        "\n",
        "train_arpa = all_clips_arpa[:train_end]\n",
        "validation_arpa = all_clips_arpa[train_end:]\n",
        "\n",
        "# dump the info to filelist files\n",
        "with open('train_filelist.txt', 'w') as train_out:\n",
        "  train_out.write('\\n'.join(train)+'\\n'+'\\n'.join(train_arpa)+\"\")\n",
        "with open('val_filelist.txt', 'w') as val_out:\n",
        "  val_out.write('\\n'.join(validation)+'\\n'+'\\n'.join(validation_arpa)+\"\")\n",
        "\n",
        "##################################################################################################################\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZ_e-QyxQMJ7",
        "outputId": "3c88f6ea-ab1d-4825-c035-4a21923026d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enabling notebook extension jupyter-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/etc/jupyter/nbconfig/notebook.json\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/etc/jupyter/nbconfig/notebook.json\n"
          ]
        }
      ],
      "source": [
        "!jupyter nbextension enable --py --sys-prefix widgetsnbextension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFLxz__HRx8o"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import librosa\n",
        "\n",
        "import torch\n",
        "import IPython.display as ipd\n",
        "from matplotlib.pyplot import imshow\n",
        "from matplotlib import pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "from nemo.collections.tts.models import FastPitchModel\n",
        "\n",
        "######################################################################################\n",
        "\n",
        "#creating the json files for training and validation\n",
        "with open('/content/train_filelist.txt') as f :\n",
        "  records = f.readlines()\n",
        "\n",
        "print(\"Number of records : \",len(records))\n",
        "\n",
        "train_manifest = 'fastpitch_train.json'\n",
        "\n",
        "train_rec = []\n",
        "random.shuffle(records)\n",
        "count = 0\n",
        "\n",
        "for i in records :\n",
        "  #if count > 1200 :\n",
        "  #  break\n",
        "  \n",
        "  i = i.split('|')\n",
        "  audio_filepath = i[0]\n",
        "  text = i[-1].strip('\\n')\n",
        "  if '{' in text :\n",
        "    #print(text)\n",
        "    continue\n",
        "  count = count + 1\n",
        "\n",
        "  duration = librosa.get_duration(filename=audio_filepath)\n",
        "  r = {\n",
        "       \"audio_filepath\" : audio_filepath,\n",
        "       \"text\" : text,\n",
        "       \"duration\" : round(duration,1),\n",
        "       \"text_no_preprocessing\" : text\n",
        "    }\n",
        "\n",
        "  train_rec.append(r)\n",
        "\n",
        "with open(train_manifest, \"w\") as f:\n",
        "    for s in train_rec:\n",
        "        f.write(json.dumps(s) + '\\n')\n",
        "        \n",
        "print(\"Training Data : \", len(train_rec))\n",
        "\n",
        "##########################################################################################\n",
        "\n",
        "with open('/content/val_filelist.txt') as f :\n",
        "  records_val = f.readlines()\n",
        "\n",
        "print(\"Number of records : \", len(records_val))\n",
        "\n",
        "val_manifest = 'fastpitch_val.json'\n",
        "count = 0\n",
        "val_rec = []\n",
        "random.shuffle(records_val)\n",
        "\n",
        "for i in records_val:\n",
        "  #if count > 50 :\n",
        "  #  break\n",
        "    \n",
        "  i = i.split('|')\n",
        "  audio_filepath = i[0]\n",
        "  text = i[-1].strip('\\n')\n",
        "\n",
        "  if '{' in text :\n",
        "    #print(text)\n",
        "    continue\n",
        "  count = count + 1\n",
        "  duration = librosa.get_duration(filename=audio_filepath)\n",
        "  r = {\n",
        "       \"audio_filepath\" : audio_filepath,\n",
        "       \"text\" : text,\n",
        "       \"duration\" : round(duration,1),\n",
        "       \"text_no_preprocessing\" : text\n",
        "    }\n",
        "\n",
        "  val_rec.append(r)\n",
        "\n",
        "print(len(val_rec))\n",
        "with open(val_manifest, \"w\") as f:\n",
        "    for s in val_rec:\n",
        "        f.write(json.dumps(s) + '\\n')\n",
        "\n",
        "####################################################################################\n",
        "FastPitchModel.from_pretrained(\"tts_en_fastpitch\")\n",
        "nemo_files = [p for p in Path(f\"{home_path}/.cache/torch/NeMo/\").glob(\"**/tts_en_fastpitch_align.nemo\")]\n",
        "print(f\"Copying {nemo_files[0]} to ./\")\n",
        "Path(\"./tts_en_fastpitch_align.nemo\").write_bytes(nemo_files[0].read_bytes())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JIDochY6UFq9",
        "outputId": "433c9403-6e3f-4dda-c307-aae3c7d08f65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-21 14:50:49--  https://raw.githubusercontent.com/nvidia/NeMo/main/examples/tts/fastpitch_finetune.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1751 (1.7K) [text/plain]\n",
            "Saving to: ‘fastpitch_finetune.py’\n",
            "\n",
            "fastpitch_finetune. 100%[===================>]   1.71K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-07-21 14:50:50 (40.1 MB/s) - ‘fastpitch_finetune.py’ saved [1751/1751]\n",
            "\n",
            "--2022-07-21 14:50:50--  https://raw.githubusercontent.com/nvidia/NeMo/main/examples/tts/conf/fastpitch_align_v1.05.yaml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6692 (6.5K) [text/plain]\n",
            "Saving to: ‘fastpitch_align_v1.05.yaml’\n",
            "\n",
            "fastpitch_align_v1. 100%[===================>]   6.54K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-07-21 14:50:50 (71.6 MB/s) - ‘fastpitch_align_v1.05.yaml’ saved [6692/6692]\n",
            "\n",
            "--2022-07-21 14:50:50--  https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/tts_dataset_files/cmudict-0.7b_nv22.07\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3722012 (3.5M) [text/plain]\n",
            "Saving to: ‘cmudict-0.7b_nv22.07’\n",
            "\n",
            "cmudict-0.7b_nv22.0 100%[===================>]   3.55M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-07-21 14:50:50 (76.6 MB/s) - ‘cmudict-0.7b_nv22.07’ saved [3722012/3722012]\n",
            "\n",
            "--2022-07-21 14:50:50--  https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/tts_dataset_files/heteronyms-030921\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3132 (3.1K) [text/plain]\n",
            "Saving to: ‘heteronyms-030921’\n",
            "\n",
            "heteronyms-030921   100%[===================>]   3.06K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-07-21 14:50:51 (65.2 MB/s) - ‘heteronyms-030921’ saved [3132/3132]\n",
            "\n",
            "--2022-07-21 14:50:51--  http://wget/\n",
            "Resolving wget (wget)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘wget’\n",
            "--2022-07-21 14:50:51--  https://raw.githubusercontent.com/NVIDIA/NeMo/main/nemo_text_processing/text_normalization/en/data/whitelist/lj_speech.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 263 [text/plain]\n",
            "Saving to: ‘lj_speech.tsv’\n",
            "\n",
            "lj_speech.tsv       100%[===================>]     263  --.-KB/s    in 0s      \n",
            "\n",
            "2022-07-21 14:50:51 (19.5 MB/s) - ‘lj_speech.tsv’ saved [263/263]\n",
            "\n",
            "FINISHED --2022-07-21 14:50:51--\n",
            "Total wall clock time: 0.2s\n",
            "Downloaded: 1 files, 263 in 0s (19.5 MB/s)\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/nvidia/NeMo/main/examples/tts/fastpitch_finetune.py\n",
        "\n",
        "!mkdir -p conf \\\n",
        "&& cd conf \\\n",
        "&& wget https://raw.githubusercontent.com/nvidia/NeMo/main/examples/tts/conf/fastpitch_align_v1.05.yaml \\\n",
        "&& cd ..\n",
        "\n",
        "#######################################\n",
        "# additional files\n",
        "!mkdir -p tts_dataset_files && cd tts_dataset_files \\\n",
        "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/tts_dataset_files/cmudict-0.7b_nv22.07 \\\n",
        "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/tts_dataset_files/heteronyms-030921 \\\n",
        "&& wget wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/nemo_text_processing/text_normalization/en/data/whitelist/lj_speech.tsv \\\n",
        "&& cd ..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYVNGY5f1nNB"
      },
      "outputs": [],
      "source": [
        "from nemo.collections.tts.torch.g2ps import EnglishG2p\n",
        "from nemo.collections.tts.torch.data import TTSDataset\n",
        "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
        "from nemo.collections.tts.torch.tts_tokenizers import EnglishPhonemesTokenizer, EnglishCharsTokenizer\n",
        "\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "# Text normalizer\n",
        "text_normalizer = Normalizer(\n",
        "    lang=\"en\", \n",
        "    input_case=\"cased\", \n",
        "    whitelist=\"./tts_dataset_files/lj_speech.tsv\"\n",
        ")\n",
        "\n",
        "text_normalizer_call_kwargs = {\n",
        "    \"punct_pre_process\": True,\n",
        "    \"punct_post_process\": True\n",
        "}\n",
        "\n",
        "# Text tokenizer\n",
        "text_tokenizer = EnglishCharsTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXn8Kvwt1nNB"
      },
      "outputs": [],
      "source": [
        "def pre_calculate_supplementary_data(sup_data_path, sup_data_types, text_tokenizer, text_normalizer, text_normalizer_call_kwargs):\n",
        "    # init train and val dataloaders\n",
        "    stages = [\"train\", \"val\"]\n",
        "    stage2dl = {}\n",
        "    for stage in stages:\n",
        "        ds = TTSDataset(\n",
        "            manifest_filepath=f\"fastpitch_{stage}.json\",\n",
        "            sample_rate=16000,\n",
        "            sup_data_path=sup_data_path,\n",
        "            sup_data_types=sup_data_types,\n",
        "            n_fft=1024,\n",
        "            win_length=1024,\n",
        "            hop_length=256,\n",
        "            window=\"hann\",\n",
        "            n_mels=80,\n",
        "            lowfreq=0,\n",
        "            highfreq=8000,\n",
        "            text_tokenizer=text_tokenizer,\n",
        "            text_normalizer=text_normalizer,\n",
        "            text_normalizer_call_kwargs=text_normalizer_call_kwargs\n",
        "\n",
        "        ) \n",
        "        stage2dl[stage] = torch.utils.data.DataLoader(ds, batch_size=1, collate_fn=ds._collate_fn, num_workers=0)\n",
        "\n",
        "    # iteration over dataloaders\n",
        "    pitch_mean, pitch_std, pitch_min, pitch_max = None, None, None, None\n",
        "    for stage, dl in stage2dl.items():\n",
        "        pitch_list = []\n",
        "        print(stage)\n",
        "        print(dl)\n",
        "        for batch in tqdm(dl, total=len(dl)):\n",
        "            tokens, tokens_lengths, audios, audio_lengths, attn_prior, pitches, pitches_lengths = batch\n",
        "            pitch = pitches.squeeze(0)\n",
        "            pitch_list.append(pitch[pitch != 0])\n",
        "\n",
        "        if stage == \"train\":\n",
        "            pitch_tensor = torch.cat(pitch_list)\n",
        "            pitch_mean, pitch_std = pitch_tensor.mean().item(), pitch_tensor.std().item()\n",
        "            pitch_min, pitch_max = pitch_tensor.min().item(), pitch_tensor.max().item()\n",
        "    \n",
        "    print(\"Pitch_mean : \",pitch_mean)\n",
        "    print(\"Pitch_std : \",pitch_std)\n",
        "    print(\"Pitch_min : \",pitch_min)\n",
        "    print(\"Pitch_max : \",pitch_max)\n",
        "            \n",
        "    return pitch_mean, pitch_std, pitch_min, pitch_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9o8yYCO1nNB"
      },
      "outputs": [],
      "source": [
        "fastpitch_sup_data_path = \"fastpitch_sup_data_folder\"\n",
        "sup_data_types = [\"align_prior_matrix\", \"pitch\"]\n",
        "\n",
        "pitch_mean, pitch_std, pitch_min, pitch_max = pre_calculate_supplementary_data(\n",
        "    fastpitch_sup_data_path, sup_data_types, text_tokenizer, text_normalizer, text_normalizer_call_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzVWRDF0lQxI"
      },
      "source": [
        "## finetuning the Fastpitch model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-Le4ulKNgGv4",
        "outputId": "6779a95c-130a-475d-a7eb-be482767d53e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "#setting up wandb credentials\n",
        "#weights and biases \n",
        "wandb_api_key = \"d06dcb679ec9cf0d71c81cb750d142b6b90f3d25\"\n",
        "wandb_project_name = \"shortFuse\"\n",
        "wandb_run_name = \"shortFuse\"\n",
        "\n",
        "!wandb login ${wandb_api_key}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#!cp /content/drive/MyDrive/TTS_finetuning/apple_jack/FastPitch--v_loss=1.1669-epoch=62-last.ckpt .\n",
        "#!mv FastPitch--v_loss=1.1669-epoch=62-last.ckpt ./Fastpitch_epoch_62.ckpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k26UuEKzFUkB",
        "outputId": "11173627-696e-43e5-e8cb-07cd366dc204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DahespoOxhqp"
      },
      "outputs": [],
      "source": [
        "#resuming the training \n",
        "#+init_from_ptl_ckpt=Fastpitch_epoch_356.ckpt\n",
        "# trainer.max_epochs=500 ~trainer.max_epochs \n",
        "#+init_from_nemo_model=tts_en_fastpitch_align.nemo \\\n",
        "! (python fastpitch_finetune.py --config-name=fastpitch_align_v1.05.yaml \\\n",
        "  train_dataset=fastpitch_train.json \\\n",
        "  validation_datasets=fastpitch_val.json \\\n",
        "  sup_data_path=fastpitch_sup_data \\\n",
        "  phoneme_dict_path=tts_dataset_files/cmudict-0.7b_nv22.07 \\\n",
        "  heteronyms_path=tts_dataset_files/heteronyms-030921 \\\n",
        "  whitelist_path=tts_dataset_files/lj_speech.tsv \\\n",
        "  exp_manager.exp_dir=training_logs \\\n",
        "  +init_from_nemo_model=tts_en_fastpitch_align.nemo \\\n",
        "  trainer.max_epochs=1000 ~trainer.max_epochs \\\n",
        "  trainer.check_val_every_n_epoch=7 \\\n",
        "  model.train_ds.dataloader_params.batch_size=8 model.validation_ds.dataloader_params.batch_size=8 \\\n",
        "  model.n_speakers=1 model.pitch_mean=324.43 model.pitch_std=112.51 \\\n",
        "  model.pitch_fmin=65.41 model.pitch_fmax=2093 model.optim.lr=2e-4 \\\n",
        "  +exp_manager.create_wandb_logger=true \\\n",
        "  +exp_manager.wandb_logger_kwargs.name=${wandb_run_name} \\\n",
        "  +exp_manager.wandb_logger_kwargs.project=${wandb_project_name} \\\n",
        "  ~model.optim.sched model.optim.name=adam trainer.devices=1 trainer.strategy=null \\\n",
        "  +model.text_tokenizer.add_blank_at=true \\\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yQ5qgAXHnbBV"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import os\n",
        "import json\n",
        "import librosa\n",
        "\n",
        "import torch\n",
        "import IPython.display as ipd\n",
        "from matplotlib.pyplot import imshow\n",
        "from matplotlib import pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "from nemo.collections.tts.models import FastPitchModel\n",
        "from nemo.collections.tts.models import HifiGanModel\n",
        "from nemo.collections.tts.torch.helpers import BetaBinomialInterpolator\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "\n",
        "#helper functions\n",
        "def infer(spec_gen_model, vocoder_model, str_input, speaker=None):\n",
        "    \"\"\"\n",
        "    Synthesizes spectrogram and audio from a text string given a spectrogram synthesis and vocoder model.\n",
        "    \n",
        "    Args:\n",
        "        spec_gen_model: Spectrogram generator model (FastPitch in our case)\n",
        "        vocoder_model: Vocoder model (HiFiGAN in our case)\n",
        "        str_input: Text input for the synthesis\n",
        "        speaker: Speaker ID\n",
        "    \n",
        "    Returns:\n",
        "        spectrogram and waveform of the synthesized audio.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        parsed = spec_gen_model.parse(str_input)\n",
        "        if speaker is not None:\n",
        "            speaker = torch.tensor([speaker]).long().to(device=spec_gen_model.device)\n",
        "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed, speaker=speaker)\n",
        "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
        "        \n",
        "    if spectrogram is not None:\n",
        "        if isinstance(spectrogram, torch.Tensor):\n",
        "            spectrogram = spectrogram.to('cpu').numpy()\n",
        "        if len(spectrogram.shape) == 3:\n",
        "            spectrogram = spectrogram[0]\n",
        "    if isinstance(audio, torch.Tensor):\n",
        "        audio = audio.to('cpu').numpy()\n",
        "    return spectrogram, audio\n",
        "\n",
        "def custom_infer(transcript, spec_model, vocoder, download=None):\n",
        "  spec, audio = infer(spec_model, vocoder, transcript)\n",
        "  print(\"\\n\\n\\n\\n\")\n",
        "  ipd.display(ipd.Audio(audio, rate=22050))\n",
        "  %matplotlib inline\n",
        "  plt.show()\n",
        "\n",
        "def try_infering_with_finetuned_fastpitch(checkpointpath, sentence):\n",
        "  spec_model = FastPitchModel.load_from_checkpoint(checkpointpath)\n",
        "  spec_model.eval().cuda()\n",
        "\n",
        "  vocoder = HifiGanModel.from_pretrained(\"tts_hifigan\")\n",
        "  vocoder = vocoder.eval().cuda()\n",
        "\n",
        "  custom_infer(sentence, spec_model, vocoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1i1ojPfpDDn"
      },
      "outputs": [],
      "source": [
        "checkpointpath = \"/content/training_logs/FastPitch/2022-07-20_15-46-30/checkpoints/FastPitch--v_loss=1.1955-epoch=97-last.ckpt\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c2= '/content/drive/MyDrive/TTS_finetuning/apple_jack/FastPitch--v_loss=1.1741-epoch=6.ckpt'\n",
        "spec_model2 = FastPitchModel.load_from_checkpoint(c2)\n",
        "spec_model2.eval().cuda()"
      ],
      "metadata": {
        "id": "cTiWUtMcJogB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJt-dJjjq1Wh"
      },
      "outputs": [],
      "source": [
        "spec_model = FastPitchModel.load_from_checkpoint(checkpointpath)\n",
        "spec_model.eval().cuda()\n",
        "\n",
        "vocoder = HifiGanModel.from_pretrained(\"tts_hifigan\")\n",
        "vocoder = vocoder.eval().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_infer(\"My money don't jiggle jiggle, it folds, make your body wiggle wiggle !\", spec_model2, vocoder)"
      ],
      "metadata": {
        "id": "Gtr8__QrJOkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdG_V1QjtTsn"
      },
      "outputs": [],
      "source": [
        "custom_infer(\"My money don't jiggle jiggle, it folds, make your body wiggle wiggle !\", spec_model, vocoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iSM-n1rM9jA"
      },
      "source": [
        "## Train for more number of epochs==> to improve upon the clarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXExc0GLq8Qo",
        "outputId": "3110017e-37b9-486c-d870-0e7b7bba9c85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU37pP1krSoY"
      },
      "outputs": [],
      "source": [
        "! cp /content/training_logs/FastPitch/2022-07-18_15-54-01/checkpoints/FastPitch--v_loss=1.1669-epoch=62-last.ckpt -t /content/drive/MyDrive/TTS_finetuning/apple_jack/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uB3yh9dPgAO"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/TTS_finetuning/apple_jack/FastPitch--v_loss=1.1669-epoch=62-last.ckpt .\n",
        "!mv FastPitch--v_loss=1.1669-epoch=62-last.ckpt ./Fastpitch_epoch_62.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSsdEPo8FLYM"
      },
      "outputs": [],
      "source": [
        "! cp /content/drive/MyDrive/TTS_finetuning/Twilight_sparkle/FastPitch--v_loss=1.2314-epoch=356-last.ckpt  -t ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVxA-IYbr8s5"
      },
      "outputs": [],
      "source": [
        "#! cp /content/training_logs2/FastPitch/2022-07-09_19-40-53/checkpoints/FastPitch--v_loss=1.2314-epoch=356-last.ckpt -t /content/\n",
        "! mv /content/training_logs/FastPitch/2022-07-18_15-54-01/checkpoints/FastPitch--v_loss=1.1669-epoch=62-last.ckpt  ./Fastpitch_epoch_62.ckpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zqAPZZolV_1"
      },
      "source": [
        "## Preparing the data for HiFiGAN finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ymqkwOOHlnoP"
      },
      "outputs": [],
      "source": [
        "#helper functions\n",
        "def load_wav(audio_file, target_sr=None):\n",
        "    with sf.SoundFile(audio_file, 'r') as f:\n",
        "        samples = f.read(dtype='float32')\n",
        "        sample_rate = f.samplerate\n",
        "        if target_sr is not None and target_sr != sample_rate:\n",
        "            samples = librosa.core.resample(samples, orig_sr=sample_rate, target_sr=target_sr)\n",
        "    return samples.transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FO_AtWobFmSp"
      },
      "outputs": [],
      "source": [
        "spec_model = FastPitchModel.load_from_checkpoint(\"/content/Fastpitch_epoch_356.ckpt\")\n",
        "spec_model.eval().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "T-oaZXnU1nND",
        "outputId": "6c69b14e-6d30-4cf5-8d4b-5563cc11766a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2022-07-12 10:49:58 nemo_logging:349] /usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\u001b[0m\n",
            "      warnings.warn(problem)\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "#dataset preparation\n",
        "# Get records from the training manifest\n",
        "manifest_path = \"./fastpitch_train.json\"\n",
        "records = []\n",
        "with open(manifest_path, \"r\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        records.append(json.loads(line))\n",
        "\n",
        "beta_binomial_interpolator = BetaBinomialInterpolator()\n",
        "spec_model.eval()\n",
        "\n",
        "device = spec_model.device\n",
        "\n",
        "save_dir = Path(\"./train_mels\")\n",
        "save_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Generate a spectrograms (we need to use ground truth alignment for correct matching between audio and mels)\n",
        "for i, r in enumerate(records):\n",
        "    audio = load_wav(r[\"audio_filepath\"])\n",
        "    audio = torch.from_numpy(audio).unsqueeze(0).to(device)\n",
        "    audio_len = torch.tensor(audio.shape[1], dtype=torch.long, device=device).unsqueeze(0)\n",
        "    \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        if \"normalized_text\" in r:\n",
        "            text = spec_model.parse(r[\"normalized_text\"], normalize=False)\n",
        "        else:\n",
        "            text = spec_model.parse(r['text'])\n",
        "        \n",
        "        text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=device).unsqueeze(0)\n",
        "    \n",
        "        spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len)\n",
        "\n",
        "        # Generate attention prior and spectrogram inputs for HiFi-GAN\n",
        "        attn_prior = torch.from_numpy(\n",
        "          beta_binomial_interpolator(spect_len.item(), text_len.item())\n",
        "        ).unsqueeze(0).to(text.device)\n",
        "            \n",
        "        spectrogram = spec_model.forward(\n",
        "          text=text, \n",
        "          input_lens=text_len, \n",
        "          spec=spect, \n",
        "          mel_lens=spect_len, \n",
        "          attn_prior=attn_prior,\n",
        "        )[0]\n",
        "        \n",
        "        save_path = save_dir / f\"mel_{i}.npy\"\n",
        "        np.save(save_path, spectrogram[0].to('cpu').numpy())\n",
        "        r[\"mel_filepath\"] = str(save_path)\n",
        "\n",
        "hifigan_manifest_path = \"hifigan_train_ft.json\"\n",
        "with open(hifigan_manifest_path, \"w\") as f:\n",
        "    for r in records:\n",
        "        f.write(json.dumps(r) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DNygV2Jg1nND"
      },
      "outputs": [],
      "source": [
        "# Get records from the training manifest\n",
        "manifest_path_validation = \"./fastpitch_val.json\"\n",
        "records_val = []\n",
        "with open(manifest_path_validation, \"r\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        records_val.append(json.loads(line))\n",
        "\n",
        "save_dir_val = Path(\"./dev_mels\")\n",
        "save_dir_val.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Generate a spectrograms (we need to use ground truth alignment for correct matching between audio and mels)\n",
        "for i, r in enumerate(records_val):\n",
        "    audio = load_wav(r[\"audio_filepath\"])\n",
        "    audio = torch.from_numpy(audio).unsqueeze(0).to(device)\n",
        "    audio_len = torch.tensor(audio.shape[1], dtype=torch.long, device=device).unsqueeze(0)\n",
        "    \n",
        "       \n",
        "    with torch.no_grad():\n",
        "        if \"normalized_text\" in r:\n",
        "            text = spec_model.parse(r[\"normalized_text\"], normalize=False)\n",
        "        else:\n",
        "            text = spec_model.parse(r['text'])\n",
        "        \n",
        "        text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=device).unsqueeze(0)\n",
        "    \n",
        "        spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len)\n",
        "\n",
        "        # Generate attention prior and spectrogram inputs for HiFi-GAN\n",
        "        attn_prior = torch.from_numpy(\n",
        "          beta_binomial_interpolator(spect_len.item(), text_len.item())\n",
        "        ).unsqueeze(0).to(text.device)\n",
        "            \n",
        "        spectrogram = spec_model.forward(\n",
        "          text=text, \n",
        "          input_lens=text_len, \n",
        "          spec=spect, \n",
        "          mel_lens=spect_len, \n",
        "          attn_prior=attn_prior,\n",
        "        )[0]\n",
        "        \n",
        "        save_path_val = save_dir_val / f\"mel_{i}.npy\"\n",
        "        np.save(save_path_val, spectrogram[0].to('cpu').numpy())\n",
        "        r[\"mel_filepath\"] = str(save_path_val)\n",
        "\n",
        "hifigan_val_manifest_path = \"hifigan_val_ft.json\"\n",
        "with open(hifigan_val_manifest_path, \"w\") as f:\n",
        "    for r in records_val:\n",
        "        f.write(json.dumps(r) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B7lL9k5Y1nND",
        "outputId": "6bddea13-99a3-4a6b-8455-aa2b07371d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  hifigan.zip\n",
            "   creating: hifigan/\n",
            "   creating: hifigan/model/\n",
            "   creating: hifigan/model/validation_ds/\n",
            " extracting: hifigan/model/validation_ds/val_ds.yaml  \n",
            " extracting: hifigan/model/validation_ds/val_ds_finetune.yaml  \n",
            "   creating: hifigan/model/train_ds/\n",
            " extracting: hifigan/model/train_ds/train_ds.yaml  \n",
            " extracting: hifigan/model/train_ds/train_ds_finetune.yaml  \n",
            "   creating: hifigan/model/generator/\n",
            " extracting: hifigan/model/generator/v1.yaml  \n",
            " extracting: hifigan/model/generator/v1_44100.yaml  \n",
            " extracting: hifigan/model/generator/v2.yaml  \n",
            " extracting: hifigan/model/generator/v3.yaml  \n",
            " extracting: hifigan/hifigan.yaml    \n",
            " extracting: hifigan/hifigan_44100.yaml  \n",
            "--2022-07-12 10:52:50--  https://raw.githubusercontent.com/nvidia/NeMo/main/examples/tts/conf/hifigan/hifigan.yaml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2244 (2.2K) [text/plain]\n",
            "Saving to: ‘hifigan.yaml.1’\n",
            "\n",
            "hifigan.yaml.1      100%[===================>]   2.19K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-07-12 10:52:50 (48.5 MB/s) - ‘hifigan.yaml.1’ saved [2244/2244]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! mv hifigan.zip -t ./conf\n",
        "! cd conf && unzip hifigan.zip\n",
        "! cd conf && cd hifigan && wget https://raw.githubusercontent.com/nvidia/NeMo/main/examples/tts/conf/hifigan/hifigan.yaml && cd .. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SkIm7L3W1nND",
        "outputId": "22dc1ae4-e9c1-4713-fcbf-19bfc9c39f80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2022-07-12 10:52:23 cloud:66] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/tts_hifigan/versions/1.0.0rc1/files/tts_hifigan.nemo to /root/.cache/torch/NeMo/NeMo_1.11.0rc0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo\n",
            "[NeMo I 2022-07-12 10:52:44 common:789] Instantiating model from pre-trained checkpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2022-07-12 10:52:47 modelPT:150] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    dataset:\n",
            "      _target_: nemo.collections.tts.data.datalayers.MelAudioDataset\n",
            "      manifest_filepath: /home/fkreuk/data/train_finetune.txt\n",
            "      min_duration: 0.75\n",
            "      n_segments: 8192\n",
            "    dataloader_params:\n",
            "      drop_last: false\n",
            "      shuffle: true\n",
            "      batch_size: 64\n",
            "      num_workers: 4\n",
            "    \n",
            "[NeMo W 2022-07-12 10:52:47 modelPT:157] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    dataset:\n",
            "      _target_: nemo.collections.tts.data.datalayers.MelAudioDataset\n",
            "      manifest_filepath: /home/fkreuk/data/val_finetune.txt\n",
            "      min_duration: 3\n",
            "      n_segments: 66150\n",
            "    dataloader_params:\n",
            "      drop_last: false\n",
            "      shuffle: false\n",
            "      batch_size: 5\n",
            "      num_workers: 4\n",
            "    \n",
            "[NeMo W 2022-07-12 10:52:47 features:178] Using torch_stft is deprecated and has been removed. The values have been forcibly set to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2022-07-12 10:52:47 features:200] PADDING: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2022-07-12 10:52:47 features:178] Using torch_stft is deprecated and has been removed. The values have been forcibly set to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2022-07-12 10:52:47 features:200] PADDING: 0\n",
            "[NeMo I 2022-07-12 10:52:49 save_restore_connector:243] Model HifiGanModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.11.0rc0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo.\n",
            "Copying /root/.cache/torch/NeMo/NeMo_1.11.0rc0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo to ./\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "315386678"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd\n",
        "HifiGanModel.from_pretrained(\"tts_hifigan\")\n",
        "nemo_files = [p for p in Path(f\"{home_path}/.cache/torch/NeMo/\").glob(\"**/tts_hifigan.nemo\")]\n",
        "print(f\"Copying {nemo_files[0]} to ./\")\n",
        "Path(\"./tts_hifigan.nemo\").write_bytes(nemo_files[0].read_bytes())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWTj8FdIloC-"
      },
      "source": [
        "## finetuning the HiFiGAN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Wv0A6OARlrVv",
        "outputId": "b5c80c57-e462-4670-afac-a49f1aedb4aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo W 2022-06-20 18:40:49 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
            "[NeMo W 2022-06-20 18:40:49 experimental:28] Module <class 'nemo.collections.tts.torch.tts_tokenizers.IPATokenizer'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "[NeMo I 2022-06-20 18:40:49 exp_manager:287] Experiments will be logged at training_logs/HifiGan/2022-06-20_18-40-49\n",
            "[NeMo I 2022-06-20 18:40:49 exp_manager:661] TensorboardLogger has been set up\n",
            "[NeMo W 2022-06-20 18:40:49 nemo_logging:349] /usr/local/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:2319: LightningDeprecationWarning: `Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.\n",
            "      rank_zero_deprecation(\"`Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.\")\n",
            "    \n",
            "[NeMo W 2022-06-20 18:40:49 exp_manager:896] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 10000. Please ensure that max_steps will run for at least 2 epochs to ensure that checkpointing will not error out.\n",
            "Created a temporary directory at /tmp/tmpahb8elqk\n",
            "Writing /tmp/tmpahb8elqk/_remote_module_non_sriptable.py\n",
            "[NeMo I 2022-06-20 18:40:50 data:846] Loading dataset from ./hifigan_train_ft.json.\n",
            "8753it [00:00, 186948.42it/s]\n",
            "[NeMo I 2022-06-20 18:40:50 data:870] Loaded dataset with 8753 files.\n",
            "[NeMo I 2022-06-20 18:40:50 data:872] Dataset contains 5.92 hours.\n",
            "[NeMo I 2022-06-20 18:40:50 data:315] Pruned 10 files. Final dataset contains 8743 files\n",
            "[NeMo I 2022-06-20 18:40:50 data:318] Pruned 0.00 hours. Final dataset contains 5.92 hours.\n",
            "[NeMo I 2022-06-20 18:40:50 data:846] Loading dataset from ./hifigan_val_ft.json.\n",
            "461it [00:00, 190687.79it/s]\n",
            "[NeMo I 2022-06-20 18:40:50 data:870] Loaded dataset with 461 files.\n",
            "[NeMo I 2022-06-20 18:40:50 data:872] Dataset contains 0.31 hours.\n",
            "[NeMo I 2022-06-20 18:40:50 data:315] Pruned 342 files. Final dataset contains 119 files\n",
            "[NeMo I 2022-06-20 18:40:50 data:318] Pruned 0.17 hours. Final dataset contains 0.13 hours.\n",
            "[NeMo I 2022-06-20 18:40:50 features:200] PADDING: 0\n",
            "[NeMo I 2022-06-20 18:40:50 features:208] STFT using exact pad\n",
            "[NeMo I 2022-06-20 18:40:50 features:200] PADDING: 0\n",
            "[NeMo I 2022-06-20 18:40:50 features:208] STFT using exact pad\n",
            "[NeMo W 2022-06-20 18:40:54 modelPT:149] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    dataset:\n",
            "      _target_: nemo.collections.tts.data.datalayers.MelAudioDataset\n",
            "      manifest_filepath: /home/fkreuk/data/train_finetune.txt\n",
            "      min_duration: 0.75\n",
            "      n_segments: 8192\n",
            "    dataloader_params:\n",
            "      drop_last: false\n",
            "      shuffle: true\n",
            "      batch_size: 64\n",
            "      num_workers: 4\n",
            "    \n",
            "[NeMo W 2022-06-20 18:40:54 modelPT:156] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    dataset:\n",
            "      _target_: nemo.collections.tts.data.datalayers.MelAudioDataset\n",
            "      manifest_filepath: /home/fkreuk/data/val_finetune.txt\n",
            "      min_duration: 3\n",
            "      n_segments: 66150\n",
            "    dataloader_params:\n",
            "      drop_last: false\n",
            "      shuffle: false\n",
            "      batch_size: 5\n",
            "      num_workers: 4\n",
            "    \n",
            "[NeMo W 2022-06-20 18:40:54 features:178] Using torch_stft is deprecated and has been removed. The values have been forcibly set to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n",
            "[NeMo I 2022-06-20 18:40:54 features:200] PADDING: 0\n",
            "[NeMo W 2022-06-20 18:40:54 features:178] Using torch_stft is deprecated and has been removed. The values have been forcibly set to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n",
            "[NeMo I 2022-06-20 18:40:54 features:200] PADDING: 0\n",
            "[NeMo I 2022-06-20 18:40:55 save_restore_connector:243] Model HifiGanModel was successfully restored from /home/deepankaracharyya/Disk2/Twilight_sparkle/tts_hifigan.nemo.\n",
            "[NeMo I 2022-06-20 18:40:55 modelPT:991] Model checkpoint restored from nemo file with path : `tts_hifigan.nemo`\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name                       | Type                     | Params\n",
            "------------------------------------------------------------------------\n",
            "0 | audio_to_melspec_precessor | FilterbankFeatures       | 0     \n",
            "1 | trg_melspec_fn             | FilterbankFeatures       | 0     \n",
            "2 | generator                  | Generator                | 13.9 M\n",
            "3 | mpd                        | MultiPeriodDiscriminator | 41.1 M\n",
            "4 | msd                        | MultiScaleDiscriminator  | 29.6 M\n",
            "5 | feature_loss               | FeatureMatchingLoss      | 0     \n",
            "6 | discriminator_loss         | DiscriminatorLoss        | 0     \n",
            "7 | generator_loss             | GeneratorLoss            | 0     \n",
            "------------------------------------------------------------------------\n",
            "84.7 M    Trainable params\n",
            "0         Non-trainable params\n",
            "84.7 M    Total params\n",
            "338.643   Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s][NeMo W 2022-06-20 18:40:59 nemo_logging:349] /usr/local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "      category=PossibleUserWarning,\n",
            "    \n",
            "Sanity Checking DataLoader 0:   0%|                       | 0/2 [00:00<?, ?it/s][NeMo W 2022-06-20 18:40:59 nemo_logging:349] /usr/local/lib/python3.7/site-packages/torch/functional.py:696: UserWarning: stft will soon require the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release. (Triggered internally at  ../aten/src/ATen/native/SpectralOps.cpp:798.)\n",
            "      normalized, onesided, return_complex)\n",
            "    \n",
            "[NeMo W 2022-06-20 18:41:01 nemo_logging:349] /usr/local/lib/python3.7/site-packages/torch/functional.py:771: UserWarning: istft will require a complex-valued input tensor in a future PyTorch release. Matching the output from stft with return_complex=True.  (Triggered internally at  ../aten/src/ATen/native/SpectralOps.cpp:950.)\n",
            "      normalized, onesided, length, return_complex)\n",
            "    \n",
            "Epoch 0:   0%|                                          | 0/547 [00:00<?, ?it/s][W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
            "[NeMo W 2022-06-20 18:41:04 nemo_logging:349] /usr/local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:231: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "      f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
            "    \n",
            "Epoch 1:  68%|██ | 376/555 [13:50<06:35,  2.21s/it, v_num=0-49, g_l1_loss=0.378]^C\n"
          ]
        }
      ],
      "source": [
        "!(python hifigan_finetune.py \\\n",
        "--config-name=hifigan.yaml \\\n",
        "model.train_ds.dataloader_params.batch_size=16 \\\n",
        "model.max_steps=10000 \\\n",
        "model.optim.lr=0.0002 \\\n",
        "~model.optim.sched \\\n",
        "train_dataset=./hifigan_train_ft.json \\\n",
        "validation_datasets=./hifigan_val_ft.json \\\n",
        "exp_manager.exp_dir=training_logs \\\n",
        "+init_from_nemo_model=tts_hifigan.nemo \\\n",
        "trainer.check_val_every_n_epoch=2 \\\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyPTseaLJykt"
      },
      "outputs": [],
      "source": [
        "!(python hifigan_finetune.py \\\n",
        "--config-name=hifigan.yaml \\\n",
        "model.train_ds.dataloader_params.batch_size=8 \\\n",
        "+model.max_epochs=1000 \\\n",
        "model.optim.lr=0.0002 \\\n",
        "~model.optim.sched \\\n",
        "train_dataset=./hifigan_train_ft.json \\\n",
        "validation_datasets=./hifigan_val_ft.json \\\n",
        "exp_manager.exp_dir=training_logs2 \\\n",
        "+init_from_nemo_model=tts_hifigan.nemo \\\n",
        "trainer.check_val_every_n_epoch=5 \\\n",
        "++exp_manager.create_wandb_logger=true \\\n",
        "+exp_manager.wandb_logger_kwargs.name=hifigan02 \\\n",
        "+exp_manager.wandb_logger_kwargs.project=MLP \\\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJkQ8fae7AqF"
      },
      "outputs": [],
      "source": [
        "!(python hifigan_finetune.py \\\n",
        "--config-name=hifigan.yaml \\\n",
        "model.train_ds.dataloader_params.batch_size=16 \\\n",
        "+model.max_epochs=1000 \\\n",
        "model.optim.lr=0.0002 \\\n",
        "~model.optim.sched \\\n",
        "train_dataset=./hifigan_train_ft.json \\\n",
        "validation_datasets=./hifigan_val_ft.json \\\n",
        "exp_manager.exp_dir=training_logs2 \\\n",
        "+init_from_ptl_ckpt=hifigan_epoch_24.ckpt \\\n",
        "trainer.check_val_every_n_epoch=5 \\\n",
        "++exp_manager.create_wandb_logger=true \\\n",
        "+exp_manager.wandb_logger_kwargs.name=${wandb_run_name} \\\n",
        "+exp_manager.wandb_logger_kwargs.project=${wandb_project_name} \\\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujIiwwrS4Jwc"
      },
      "outputs": [],
      "source": [
        "!cp /content/training_logs2/HifiGan/2022-07-12_10-55-28/checkpoints/HifiGan--val_loss=0.3945-epoch=164-last.ckpt -t /content/drive/MyDrive/TTS_finetuning/Twilight_sparkle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFlThycmlvZg"
      },
      "source": [
        "## Copy the final weights to the gdrive for future reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1AzFLttCmTTx"
      },
      "outputs": [],
      "source": [
        "checkpoint_fp_path = \"/content/Fastpitch_epoch_356.ckpt\"\n",
        "checkpoint_hf_path = '/content/training_logs2/HifiGan/2022-07-12_10-55-28/checkpoints/HifiGan--val_loss=0.3945-epoch=164-last.ckpt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oy51rznK1nNE"
      },
      "outputs": [],
      "source": [
        "!cp /content/training_logs2/HifiGan/2022-07-11_07-28-01/checkpoints/HifiGan--val_loss=0.4430-epoch=29-last.ckpt -t /content/drive/MyDrive/TTS_finetuning/Twilight_sparkle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_j20R3j6pkz"
      },
      "outputs": [],
      "source": [
        "!cp /content/training_logs2/HifiGan/2022-07-11_07-28-01/checkpoints/HifiGan--val_loss=0.4227-epoch=24.ckpt .\n",
        "!mv HifiGan--val_loss=0.4227-epoch=24.ckpt hifigan_epoch_24.ckpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgV3Rtp_mUPh"
      },
      "source": [
        "## Load the checkpoints and do the inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GuOrHb5f1nNE"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import os\n",
        "import json\n",
        "import librosa\n",
        "\n",
        "import torch\n",
        "import IPython.display as ipd\n",
        "from matplotlib.pyplot import imshow\n",
        "from matplotlib import pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "from nemo.collections.tts.models import FastPitchModel\n",
        "from nemo.collections.tts.models import HifiGanModel\n",
        "from nemo.collections.tts.torch.helpers import BetaBinomialInterpolator\n",
        "import soundfile as sf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4DDjmqXr1nNF",
        "outputId": "78d13a8a-5fa2-4387-cee1-763bdebdaa4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2022-07-13 04:49:37 tokenize_and_classify:87] Creating ClassifyFst grammars.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2022-07-13 04:50:03 g2ps:87] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.\n",
            "[NeMo W 2022-07-13 04:50:03 modelPT:150] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    dataset:\n",
            "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
            "      manifest_filepath: fastpitch_train.json\n",
            "      sample_rate: 22050\n",
            "      sup_data_path: fastpitch_sup_data\n",
            "      sup_data_types:\n",
            "      - align_prior_matrix\n",
            "      - pitch\n",
            "      n_fft: 1024\n",
            "      win_length: 1024\n",
            "      hop_length: 256\n",
            "      window: hann\n",
            "      n_mels: 80\n",
            "      lowfreq: 0\n",
            "      highfreq: 8000\n",
            "      max_duration: null\n",
            "      min_duration: 0.1\n",
            "      ignore_file: null\n",
            "      trim: false\n",
            "      pitch_fmin: 65.41\n",
            "      pitch_fmax: 2068.96\n",
            "      pitch_norm: true\n",
            "      pitch_mean: 319.86\n",
            "      pitch_std: 122.54\n",
            "      use_beta_binomial_interpolator: true\n",
            "    dataloader_params:\n",
            "      drop_last: false\n",
            "      shuffle: true\n",
            "      batch_size: 8\n",
            "      num_workers: 12\n",
            "      pin_memory: true\n",
            "    \n",
            "[NeMo W 2022-07-13 04:50:03 modelPT:157] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    dataset:\n",
            "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
            "      manifest_filepath: fastpitch_val.json\n",
            "      sample_rate: 22050\n",
            "      sup_data_path: fastpitch_sup_data\n",
            "      sup_data_types:\n",
            "      - align_prior_matrix\n",
            "      - pitch\n",
            "      n_fft: 1024\n",
            "      win_length: 1024\n",
            "      hop_length: 256\n",
            "      window: hann\n",
            "      n_mels: 80\n",
            "      lowfreq: 0\n",
            "      highfreq: 8000\n",
            "      max_duration: null\n",
            "      min_duration: null\n",
            "      ignore_file: null\n",
            "      trim: false\n",
            "      pitch_fmin: 65.41\n",
            "      pitch_fmax: 2068.96\n",
            "      pitch_norm: true\n",
            "      pitch_mean: 319.86\n",
            "      pitch_std: 122.54\n",
            "      use_beta_binomial_interpolator: true\n",
            "    dataloader_params:\n",
            "      drop_last: false\n",
            "      shuffle: false\n",
            "      batch_size: 8\n",
            "      num_workers: 8\n",
            "      pin_memory: true\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2022-07-13 04:50:03 features:200] PADDING: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2022-07-13 04:50:04 modelPT:150] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    dataset:\n",
            "      _target_: nemo.collections.tts.torch.data.VocoderDataset\n",
            "      manifest_filepath: ./hifigan_train_ft.json\n",
            "      sample_rate: 22050\n",
            "      n_segments: 8192\n",
            "      max_duration: null\n",
            "      min_duration: 0.75\n",
            "    dataloader_params:\n",
            "      drop_last: false\n",
            "      shuffle: true\n",
            "      batch_size: 8\n",
            "      num_workers: 4\n",
            "    \n",
            "[NeMo W 2022-07-13 04:50:04 modelPT:157] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    dataset:\n",
            "      _target_: nemo.collections.tts.torch.data.VocoderDataset\n",
            "      manifest_filepath: ./hifigan_val_ft.json\n",
            "      sample_rate: 22050\n",
            "      n_segments: 66048\n",
            "      max_duration: null\n",
            "      min_duration: 3\n",
            "    dataloader_params:\n",
            "      drop_last: false\n",
            "      shuffle: false\n",
            "      batch_size: 16\n",
            "      num_workers: 1\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2022-07-13 04:50:04 features:200] PADDING: 0\n",
            "[NeMo I 2022-07-13 04:50:04 features:208] STFT using exact pad\n",
            "[NeMo I 2022-07-13 04:50:04 features:200] PADDING: 0\n",
            "[NeMo I 2022-07-13 04:50:04 features:208] STFT using exact pad\n"
          ]
        }
      ],
      "source": [
        "spec_model = FastPitchModel.load_from_checkpoint(checkpoint_fp_path)\n",
        "spec_model.eval().cuda()\n",
        "\n",
        "vocoder = HifiGanModel.load_from_checkpoint(checkpoint_hf_path)\n",
        "vocoder = vocoder.eval().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ34hgmT1nNF"
      },
      "outputs": [],
      "source": [
        "custom_infer(\"Is my voice approaching the threshold it is expected to reach?\", spec_model, vocoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiEJ3xtK1nNF"
      },
      "outputs": [],
      "source": [
        "custom_infer(\"Ouch! I got my hand broken!\", spec_model, vocoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6eslozt5YlI"
      },
      "outputs": [],
      "source": [
        "custom_infer(\"Trying out the model for the first time and I cannot contain my excitement!\", spec_model, vocoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bE7AoAl5gEb"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "TTS_finetuning_shortFuse.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}