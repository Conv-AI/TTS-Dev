{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetuning-template.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the environment"
      ],
      "metadata": {
        "id": "d3Ompkqwlrl-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5YpHvnAlmYa"
      },
      "outputs": [],
      "source": [
        "!pip install hydra-core==1.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy==1.7.3\n",
        "\n",
        "BRANCH = 'main'\n",
        "# If you're using Google Colab and not running locally, uncomment and run this cell.\n",
        "!apt-get install sox libsndfile1 ffmpeg\n",
        "!pip install wget unidecode pynini==2.1.4\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
        "\n",
        "#=== load the repo and data (Thanks Synthbot) ===\n",
        "!sudo dpkg --configure -a\n",
        "!sudo apt -qq install -y sox\n",
        "!git clone \"https://github.com/synthbot-anon/synthbot.git\" /content/synthbot\n",
        "!(cd /content/synthbot; git checkout experimental)\n",
        "!python3 -m pip install pysoundfile \n",
        "!python3 -m pip install librosa\n",
        "\n",
        "!jupyter nbextension enable --py --sys-prefix widgetsnbextension"
      ],
      "metadata": {
        "id": "_B1A2txhl7XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "home_path = !(echo $HOME)\n",
        "home_path = home_path[0]\n",
        "print(home_path)"
      ],
      "metadata": {
        "id": "2waaRXRsl-75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading the dataset\n",
        "id='12ClseQAzkqsqCNMClmptvbSjfs2Ulcei'\n",
        "character='applejack'\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id={id}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id={id}\" -O {character+'.tar'} && rm -rf /tmp/cookies.txt\n"
      ],
      "metadata": {
        "id": "kg62xObNm9Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skip_noisy = True # Disable to train with Noisy Data included\n",
        "allowed_emotions = \"\"\"\n",
        "Anxious\n",
        "Angry\n",
        "Annoyed\n",
        "Amused\n",
        "Confused\n",
        "Crazy\n",
        "Disgust\n",
        "Exhausted\n",
        "Fear\n",
        "Happy\n",
        "Neutral\n",
        "Sad\n",
        "Serious\n",
        "Singing\n",
        "Shouting\n",
        "Surprised\n",
        "Smug\n",
        "Love\n",
        "Sarcastic\n",
        "Tired\n",
        "Whispering\n",
        "Whining\n",
        "\"\"\".split(\"\\n\")[1:-1]\n",
        "\n",
        "percentage_training_data = 0.95 # 90% of Data will be used for training, 5% for Validation\n",
        "\n",
        "##############################################################################################\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/synthbot/src')\n",
        "from ponysynth.corpus import ClipperArchive, phoneme_transcription\n",
        "import librosa\n",
        "import subprocess\n",
        "\n",
        "archive_fn = \"/content/shortFuse.tar\"\n",
        "archive = ClipperArchive(archive_fn)\n",
        "\n",
        "data_path = 'shortFuse'\n",
        "allowed_emotions = [x.lower() for x in allowed_emotions]\n",
        "!mkdir {data_path}\n",
        "!mkdir {data_path+\"/out\"}\n",
        "all_clips = []; all_clips_arpa = []; skipped_count=0; too_short_count=0; emotion_skip=0\n",
        "for key in archive.keys(): # write the audio files for processing in bash terminal\n",
        "  audio = archive.read_audio(key)\n",
        "  audio_fn = '{}/{}.wav'.format(data_path, key)\n",
        "  audio_fn_ = '{}/{}.wav'.format(data_path+\"/out\", key)\n",
        "  with open(audio_fn, 'wb') as audio_out:\n",
        "    audio_out.write(audio.read())"
      ],
      "metadata": {
        "id": "g9X5Hi66mLTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script bash\n",
        "# trim all 48Khz files\n",
        "cd {character}\n",
        "for input in *.wav; do\n",
        "  output=\"out/$input\"\n",
        "  sox \"$input\" \"$output\" silence 1 0.05 0.1% reverse silence 1 0.05 0.1% reverse;\n",
        "done"
      ],
      "metadata": {
        "id": "dOgHN58_pHnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "should_continue = 0 # should run \"continue\" command inside outer loop.\n",
        "for key in archive.keys():\n",
        "  label = archive.read_label(key)\n",
        "  audio_fn = '{}/{}.wav'.format(data_path, key)\n",
        "  audio_fn_ = '{}/{}.wav'.format(data_path+\"/out\", key)\n",
        "  if (label['noise'] in ['Very Noisy','Noisy']) and skip_noisy: os.remove(audio_fn_); skipped_count+=1; continue\n",
        "\n",
        "  for tag in label['tags']:\n",
        "     if tag.lower() not in allowed_emotions:\n",
        "       try: os.remove(audio_fn_)\n",
        "       except: pass\n",
        "       print(tag+\" emotion not in list\"); emotion_skip+=1; should_continue = 1; break # this is supposed to break the outer loop\n",
        "  if should_continue: should_continue = 0; continue\n",
        "\n",
        "  audio = archive.read_audio(key)\n",
        "  transcript = label['utterance']['content']\n",
        "  \n",
        "  if os.stat(audio_fn_).st_size < 71602:\n",
        "    #print(\"Skipping Audio, Duration: \"+str(len(librosa.core.load(audio_fn_, sr=48000)[0])/48000))\n",
        "    try: os.remove(audio_fn_)\n",
        "    except: pass\n",
        "    too_short_count+=1; continue # Skips files based on size.\n",
        "\n",
        "  filelist_line = \"{}|{}\".format(audio_fn_, transcript)\n",
        "  all_clips.append(filelist_line)\n",
        "  filelist_line = \"{}|{}\".format(audio_fn_, phoneme_transcription(label))\n",
        "  all_clips_arpa.append(filelist_line)\n",
        "    \n",
        "print(str(skipped_count)+\" Files are too Noisy.\")\n",
        "print(str(emotion_skip)+\" Files contain an emotion not in permitted emotions\")\n",
        "print(str(too_short_count)+\" Files are too short\")\n",
        "print(str(len(list(archive.keys()))-(skipped_count+too_short_count+emotion_skip))+\" Files kept in dataset.\")\n",
        "\n",
        "\n",
        "####################################################################################################################\n",
        "# shuffle the training data\n",
        "import random\n",
        "random.seed(0)\n",
        "random.shuffle(all_clips)\n",
        "\n",
        "# get train, test, validation splits\n",
        "num_clips = len(all_clips)\n",
        "train_end = int(num_clips * percentage_training_data)\n",
        "\n",
        "train = all_clips[:train_end]\n",
        "validation = all_clips[train_end:]\n",
        "\n",
        "train_arpa = all_clips_arpa[:train_end]\n",
        "validation_arpa = all_clips_arpa[train_end:]\n",
        "\n",
        "# dump the info to filelist files\n",
        "with open('train_filelist.txt', 'w') as train_out:\n",
        "  train_out.write('\\n'.join(train)+'\\n'+'\\n'.join(train_arpa)+\"\")\n",
        "with open('val_filelist.txt', 'w') as val_out:\n",
        "  val_out.write('\\n'.join(validation)+'\\n'+'\\n'.join(validation_arpa)+\"\")\n",
        "\n",
        "##################################################################################################################\n",
        "\n",
        "#imports\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import librosa\n",
        "\n",
        "import torch\n",
        "import IPython.display as ipd\n",
        "from matplotlib.pyplot import imshow\n",
        "from matplotlib import pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "from nemo.collections.tts.models import FastPitchModel\n",
        "\n",
        "######################################################################################\n",
        "\n",
        "#creating the json files for training and validation\n",
        "with open('/content/train_filelist.txt') as f :\n",
        "  records = f.readlines()\n",
        "\n",
        "print(\"Number of records : \",len(records))\n",
        "\n",
        "train_manifest = 'fastpitch_train.json'\n",
        "\n",
        "train_rec = []\n",
        "random.shuffle(records)\n",
        "count = 0\n",
        "\n",
        "for i in records :\n",
        "  #if count > 1200 :\n",
        "  #  break\n",
        "  \n",
        "  i = i.split('|')\n",
        "  audio_filepath = i[0]\n",
        "  text = i[-1].strip('\\n')\n",
        "  if '{' in text :\n",
        "    #print(text)\n",
        "    continue\n",
        "  count = count + 1\n",
        "\n",
        "  duration = librosa.get_duration(filename=audio_filepath)\n",
        "  r = {\n",
        "       \"audio_filepath\" : audio_filepath,\n",
        "       \"text\" : text,\n",
        "       \"duration\" : round(duration,1),\n",
        "       \"text_no_preprocessing\" : text\n",
        "    }\n",
        "\n",
        "  train_rec.append(r)\n",
        "\n",
        "with open(train_manifest, \"w\") as f:\n",
        "    for s in train_rec:\n",
        "        f.write(json.dumps(s) + '\\n')\n",
        "        \n",
        "print(\"Training Data : \", len(train_rec))\n",
        "\n",
        "##########################################################################################\n",
        "\n",
        "with open('/content/val_filelist.txt') as f :\n",
        "  records_val = f.readlines()\n",
        "\n",
        "print(\"Number of records : \", len(records_val))\n",
        "\n",
        "val_manifest = 'fastpitch_val.json'\n",
        "count = 0\n",
        "val_rec = []\n",
        "random.shuffle(records_val)\n",
        "\n",
        "for i in records_val:\n",
        "  #if count > 50 :\n",
        "  #  break\n",
        "    \n",
        "  i = i.split('|')\n",
        "  audio_filepath = i[0]\n",
        "  text = i[-1].strip('\\n')\n",
        "\n",
        "  if '{' in text :\n",
        "    #print(text)\n",
        "    continue\n",
        "  count = count + 1\n",
        "  duration = librosa.get_duration(filename=audio_filepath)\n",
        "  r = {\n",
        "       \"audio_filepath\" : audio_filepath,\n",
        "       \"text\" : text,\n",
        "       \"duration\" : round(duration,1),\n",
        "       \"text_no_preprocessing\" : text\n",
        "    }\n",
        "\n",
        "  val_rec.append(r)\n",
        "\n",
        "print(len(val_rec))\n",
        "with open(val_manifest, \"w\") as f:\n",
        "    for s in val_rec:\n",
        "        f.write(json.dumps(s) + '\\n')\n",
        "\n",
        "####################################################################################\n",
        "\n",
        "FastPitchModel.from_pretrained(\"tts_en_fastpitch\")\n",
        "nemo_files = [p for p in Path(f\"{home_path}/.cache/torch/NeMo/\").glob(\"**/tts_en_fastpitch_align.nemo\")]\n",
        "print(f\"Copying {nemo_files[0]} to ./\")\n",
        "Path(\"./tts_en_fastpitch_align.nemo\").write_bytes(nemo_files[0].read_bytes())\n",
        "\n",
        "###################################################################################\n",
        "\n",
        "!wget https://raw.githubusercontent.com/nvidia/NeMo/main/examples/tts/fastpitch_finetune.py\n",
        "\n",
        "!mkdir -p conf \\\n",
        "&& cd conf \\\n",
        "&& wget https://raw.githubusercontent.com/nvidia/NeMo/main/examples/tts/conf/fastpitch_align_v1.05.yaml \\\n",
        "&& cd ..\n",
        "\n",
        "#######################################\n",
        "# additional files\n",
        "!mkdir -p tts_dataset_files && cd tts_dataset_files \\\n",
        "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/tts_dataset_files/cmudict-0.7b_nv22.07 \\\n",
        "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/tts_dataset_files/heteronyms-030921 \\\n",
        "&& wget wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/nemo_text_processing/text_normalization/en/data/whitelist/lj_speech.tsv \\\n",
        "&& cd ..\n",
        "\n"
      ],
      "metadata": {
        "id": "1e4fxqLTpLx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nemo.collections.tts.torch.g2ps import EnglishG2p\n",
        "from nemo.collections.tts.torch.data import TTSDataset\n",
        "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
        "from nemo.collections.tts.torch.tts_tokenizers import EnglishPhonemesTokenizer, EnglishCharsTokenizer\n",
        "\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "# Text normalizer\n",
        "text_normalizer = Normalizer(\n",
        "    lang=\"en\", \n",
        "    input_case=\"cased\", \n",
        "    whitelist=\"./tts_dataset_files/lj_speech.tsv\"\n",
        ")\n",
        "\n",
        "text_normalizer_call_kwargs = {\n",
        "    \"punct_pre_process\": True,\n",
        "    \"punct_post_process\": True\n",
        "}\n",
        "\n",
        "# Text tokenizer\n",
        "text_tokenizer = EnglishCharsTokenizer()\n",
        "\n",
        "def pre_calculate_supplementary_data(sup_data_path, sup_data_types, text_tokenizer, text_normalizer, text_normalizer_call_kwargs):\n",
        "    # init train and val dataloaders\n",
        "    stages = [\"train\", \"val\"]\n",
        "    stage2dl = {}\n",
        "    for stage in stages:\n",
        "        ds = TTSDataset(\n",
        "            manifest_filepath=f\"fastpitch_{stage}.json\",\n",
        "            sample_rate=16000,\n",
        "            sup_data_path=sup_data_path,\n",
        "            sup_data_types=sup_data_types,\n",
        "            n_fft=1024,\n",
        "            win_length=1024,\n",
        "            hop_length=256,\n",
        "            window=\"hann\",\n",
        "            n_mels=80,\n",
        "            lowfreq=0,\n",
        "            highfreq=8000,\n",
        "            text_tokenizer=text_tokenizer,\n",
        "            text_normalizer=text_normalizer,\n",
        "            text_normalizer_call_kwargs=text_normalizer_call_kwargs\n",
        "\n",
        "        ) \n",
        "        stage2dl[stage] = torch.utils.data.DataLoader(ds, batch_size=1, collate_fn=ds._collate_fn, num_workers=0)\n",
        "\n",
        "    # iteration over dataloaders\n",
        "    pitch_mean, pitch_std, pitch_min, pitch_max = None, None, None, None\n",
        "    for stage, dl in stage2dl.items():\n",
        "        pitch_list = []\n",
        "        print(stage)\n",
        "        print(dl)\n",
        "        for batch in tqdm(dl, total=len(dl)):\n",
        "            tokens, tokens_lengths, audios, audio_lengths, attn_prior, pitches, pitches_lengths = batch\n",
        "            pitch = pitches.squeeze(0)\n",
        "            pitch_list.append(pitch[pitch != 0])\n",
        "\n",
        "        if stage == \"train\":\n",
        "            pitch_tensor = torch.cat(pitch_list)\n",
        "            pitch_mean, pitch_std = pitch_tensor.mean().item(), pitch_tensor.std().item()\n",
        "            pitch_min, pitch_max = pitch_tensor.min().item(), pitch_tensor.max().item()\n",
        "    \n",
        "    print(\"Pitch_mean : \",pitch_mean)\n",
        "    print(\"Pitch_std : \",pitch_std)\n",
        "    print(\"Pitch_min : \",pitch_min)\n",
        "    print(\"Pitch_max : \",pitch_max)\n",
        "            \n",
        "    return pitch_mean, pitch_std, pitch_min, pitch_max\n",
        "\n",
        "fastpitch_sup_data_path = \"fastpitch_sup_data_folder\"\n",
        "sup_data_types = [\"align_prior_matrix\", \"pitch\"]\n",
        "\n",
        "pitch_mean, pitch_std, pitch_min, pitch_max = pre_calculate_supplementary_data(\n",
        "    fastpitch_sup_data_path, sup_data_types, text_tokenizer, text_normalizer, text_normalizer_call_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "oEG1JWKYpQDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up wandb credentials\n",
        "#weights and biases \n",
        "wandb_api_key = \"d06dcb679ec9cf0d71c81cb750d142b6b90f3d25\"\n",
        "wandb_project_name = 'MLP'\n",
        "wandb_run_name = character\n",
        "\n",
        "!wandb login ${wandb_api_key}"
      ],
      "metadata": {
        "id": "_zgg6-_yp_ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#resuming the training \n",
        "#+init_from_ptl_ckpt=Fastpitch_epoch_356.ckpt\n",
        "# trainer.max_epochs=500 ~trainer.max_epochs \n",
        "#+init_from_nemo_model=tts_en_fastpitch_align.nemo \\\n",
        "! (python fastpitch_finetune.py --config-name=fastpitch_align_v1.05.yaml \\\n",
        "  train_dataset=fastpitch_train.json \\\n",
        "  validation_datasets=fastpitch_val.json \\\n",
        "  sup_data_path=fastpitch_sup_data \\\n",
        "  phoneme_dict_path=tts_dataset_files/cmudict-0.7b_nv22.07 \\\n",
        "  heteronyms_path=tts_dataset_files/heteronyms-030921 \\\n",
        "  whitelist_path=tts_dataset_files/lj_speech.tsv \\\n",
        "  exp_manager.exp_dir=training_logs \\\n",
        "  +init_from_nemo_model=tts_en_fastpitch_align.nemo \\\n",
        "  trainer.max_epochs=1000 ~trainer.max_epochs \\\n",
        "  trainer.check_val_every_n_epoch=7 \\\n",
        "  model.train_ds.dataloader_params.batch_size=8 model.validation_ds.dataloader_params.batch_size=8 \\\n",
        "  model.n_speakers=1 model.pitch_mean=324.43 model.pitch_std=112.51 \\\n",
        "  model.pitch_fmin=65.41 model.pitch_fmax=2093 model.optim.lr=2e-4 \\\n",
        "  +exp_manager.create_wandb_logger=true \\\n",
        "  +exp_manager.wandb_logger_kwargs.name=${wandb_run_name} \\\n",
        "  +exp_manager.wandb_logger_kwargs.project=${wandb_project_name} \\\n",
        "  ~model.optim.sched model.optim.name=adam trainer.devices=1 trainer.strategy=null \\\n",
        "  +model.text_tokenizer.add_blank_at=true \\\n",
        ")"
      ],
      "metadata": {
        "id": "4sP5fIruqFX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the checkpoint"
      ],
      "metadata": {
        "id": "O0bFsK1Sqy8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = ''"
      ],
      "metadata": {
        "id": "TRSdd_GjqwVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spec_model = FastPitchModel.load_from_checkpoint(checkpointpath)\n",
        "spec_model.eval().cuda()\n",
        "\n",
        "vocoder = HifiGanModel.from_pretrained(\"tts_hifigan\")\n",
        "vocoder = vocoder.eval().cuda()"
      ],
      "metadata": {
        "id": "FfoFyCWQrKJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_infer(\"My money don't jiggle jiggle, it folds, make your body wiggle wiggle !\", spec_model2, vocoder)"
      ],
      "metadata": {
        "id": "_QMuMZPzrOJD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}